{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average shortest path length: 3.45\n",
      "Network diameter: 14\n"
     ]
    }
   ],
   "source": [
    "# Exercício 1\n",
    "G= G=nx.read_edgelist(\"data/hamsterster.txt\", nodetype=int)\n",
    "G = G.to_undirected()\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "\n",
    "mean_length = nx.average_shortest_path_length(G)\n",
    "print(\"Average shortest path length:\", \"%3.2f\"%mean_length)\n",
    "\n",
    "d = nx.diameter(G)\n",
    "print('Network diameter:', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average shortest path length: 2.9910300601202406\n",
      "Variance shortest path length: 0.8337684376959998\n"
     ]
    }
   ],
   "source": [
    "# Exercício 2\n",
    "G= G=nx.read_edgelist(\"data/USairport500.txt\", nodetype=int)\n",
    "G = G.to_undirected()\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "\n",
    "mean_length = nx.average_shortest_path_length(G)\n",
    "print(\"Average shortest path length:\", mean_length)\n",
    "\n",
    "paths = dict(nx.shortest_path_length(G))\n",
    "lengths = list(itertools.chain.from_iterable(paths[u].values() for u in paths))\n",
    "variance_length = np.var(lengths)\n",
    "print(\"Variance shortest path length:\", variance_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assortativity =  -0.0846\n"
     ]
    }
   ],
   "source": [
    "# Exercício 3\n",
    "G= G=nx.read_edgelist(\"data/advogato.txt\", nodetype=int)\n",
    "G = G.to_undirected()\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "\n",
    "r=nx.degree_assortativity_coefficient(G)\n",
    "print(\"Assortativity = \",\"%3.4f\"%r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shannon Entropy =  1.90\n"
     ]
    }
   ],
   "source": [
    "# Exercício 4\n",
    "G= G=nx.read_edgelist(\"data/USairport500.txt\", nodetype=int)\n",
    "G = G.to_undirected()\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "\n",
    "def degree_distribution(G):\n",
    "    paths = dict(nx.shortest_path_length(G))\n",
    "    lengths = list(itertools.chain.from_iterable(paths[u].values() for u in paths))\n",
    "    maxk = np.max(lengths)\n",
    "    mink = np.min(lengths)\n",
    "    lvalues= np.arange(mink,maxk+1) # possible values of k\n",
    "    Plength = np.zeros(maxk+1) # P(k)\n",
    "    for k in lengths:\n",
    "        Plength[k] = Plength[k] + 1\n",
    "    Plength = Plength/sum(Plength) # the sum of the elements of P(k) must to be equal to one\n",
    "    return lvalues,Plength\n",
    "\n",
    "def shannon_entropy(G):\n",
    "    l,Pl = degree_distribution(G)\n",
    "    H = 0\n",
    "    for p in Pl:\n",
    "        if(p > 0):\n",
    "            H = H - p*math.log(p, 2)\n",
    "    return H\n",
    "\n",
    "H = shannon_entropy(G)\n",
    "print(\"Shannon Entropy = \", \"%3.2f\"%H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation coefficient: -0.710832214935246\n"
     ]
    }
   ],
   "source": [
    "# Exercício 5\n",
    "G= G=nx.read_edgelist(\"data/word_adjacencies.txt\", nodetype=int)\n",
    "G = G.to_undirected()\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "G = G.subgraph(Gcc[0])\n",
    "\n",
    "knn = []\n",
    "for i in G.nodes():\n",
    "    aux =  nx.average_neighbor_degree(G, nodes = [i])\n",
    "    knn.append(float(aux[i]))\n",
    "knn = np.array(knn)\n",
    "\n",
    "vk = dict(G.degree())\n",
    "vk = list(vk.values())\n",
    "\n",
    "knnk = list()\n",
    "ks = list()\n",
    "for k in np.arange(np.min(vk), np.max(vk)+1):\n",
    "    aux = vk == k\n",
    "    if(len(knn[aux]) > 0):\n",
    "        av_knn = np.mean(knn[aux]) #average clustering among all the nodes with degree k\n",
    "        knnk.append(av_knn)\n",
    "        ks.append(k)\n",
    "\n",
    "# calculate Pearson's correlation\n",
    "rho = np.corrcoef(ks, knnk)[0,1]\n",
    "print('Pearson correlation coefficient:', rho)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
